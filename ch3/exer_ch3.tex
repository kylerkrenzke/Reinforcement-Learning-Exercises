\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}

\title{Chapter 3 Exercises}
\author{Kyler Krenzke}
\date{}

\begin{document}

	\maketitle
	
	\begin{enumerate}
	
		\item \textbf{Exercise 3.1: Devise three example tasks of your own that fit into the MDP framework, identifying for each its states, actions, and rewards. Make the three
		examples as different from each other as possible. The framework is abstract and flexible and can be applied in many different ways. Stretch its limits in some way in at
		least one of your examples.}
		\begin{enumerate}
			\item \textit{Resistance training routine}: The state would be defined over the hidden variables of the human musculature. The actions would be sets/exercises to try
			to increase your muscular strength, endurance, size, etc. Rewards would be given proportional to the individual's increase in exercise benchmarks (i.e. bench press 1rm
			going up by 5lb could reward +5)
			\item \textit{Rocket League player}: The state space for Rocket League is defined as the position and velocity vectors for all the players and the ball, as well as
			game information such as the score and time remaining. The action space is the possible control inputs that can be used. The rewards would be given for winning the
			game.
			\item \textit{TV programming scheduler}: The state space is a set of possible TV show episodes, advertisers/commercials, and time slots to fit it all in. The action
			space is filled with selections of certain shows or advertisements at different times. Rewards could be given proportional to the number of total view time (viewers *
			time watched) to maximize advertising revenue.
		\end{enumerate}
		
		\item \textbf{Exercise 3.2: Is the MDP framework adequate to usefully represent all goal-directed learning tasks? Can you think of any clear exceptions?}
		
		The MDP framework is adequate for all goal-directed learning tasks.
		
		\item \textbf{Exercise 3.3: Consider the problem of driving. You could define the actions in terms of the accelerator, steering wheel, and brake, that is, where your body
		meets the machine. Or you could define them farther out—say, where the rubber meets the road, considering your actions to be tire torques. Or you could define them farther
		in—say, where your brain meets your body, the actions being muscle twitches to control your limbs. Or you could go to a really high level and say that your actions are
		your choices of where to drive. What is the right level, the right place to draw the line between agent and environment? On what basis is one location of the line to be
		preferred over another? Is there any fundamental reason for preferring one location over another, or is it a free choice?}
		
		The decision of where to define the action space depends on what the agent has absolute control over while also maximizing the decisions that the agent can make to give it
		the most control.
		
		\item \textbf{Exercise 3.4: Give a table analogous to that in Example 3.3, but for $p(s',r|s,a)$. It should have columns for $s$, $a$, $r$, $s'$, and $p(s',r|s,a)$, and a
		row for every 4-tuple for which $p(s',r|s,a) > 0$.}
		
		\item \textbf{Exercise 3.5: The equations in Section 3.1 are for the continuing case and need to be modified (very slightly) to apply to episodic tasks. Show that you know
		the modifications needed by giving the modified version of (3.3).}
		
		\item \textbf{Exercise 3.6: Suppose you treated pole-balancing as an episodic task but also used discounting, with all rewards zero except for 1 upon failure. What then
		would the return be at each time? How does this return differ from that in the discounted, continuing formulation of this task?}
		
		\item \textbf{Exercise 3.7: Imagine that you are designing a robot to run a maze. You decide to give it a reward of +1 for escaping from the maze and a reward of zero at
		all other times. The task seems to break down naturally into episodes—the successive runs through the maze—so you decide to treat it as an episodic task, where the goal is
		to maximize expected total reward (3.7). After running the learning agent for a while, you find that it is showing no improvement in escaping from the maze. What is going
		wrong? Have you effectively communicated to the agent what you want it to achieve?}
		
		The problem in this case is that the agent has no incentive to find a faster route. No matter what path the agent selects, it receives the same rewards +1. To fix this, one
		possible solution could be to assign a small negative reward at every non-terminal state.
		
		\item \textbf{Exercise 3.8: Suppose $\gamma=0.5$ and the following sequence of rewards is $R_1=-1$, $R_2=2$, $R_3=6$, $R_4=3$, and $R_5=2$, with $T=5$. What are $G_0$,
		$G_1$, ..., $G_5$? Hint: Work backwards.}
		
		\item \textbf{Exercise 3.9: Suppose $\gamma=0.9$ and the reward sequence is $R_1=2$ followed by an infinite sequence of 7s. What are $G_1$ and $G_0$?}
		
		\item \textbf{Exercise 3.10: Prove the second equality in (3.10).}
	
	\end{enumerate}

\end{document}